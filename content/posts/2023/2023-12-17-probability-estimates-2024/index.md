---
title: "Probability Estimates: 2024"
slug: probability-estimates-2024
layout: post
date: 2023-12-17
tags:
- Probability
- World-Events
summary: "Ideally, when we say we think something is likely (or unlikely), we should revisit our prediction later once we know the actual outcome... so this year some friends and I will be competing by predicting various 2024 events."
---

Ideally, when we say we think something is likely (or unlikely), we should revisit our prediction later once we know the actual outcome.
It's nice to be right.

However, sometimes it's understandable to get the outcome wrong if we correctly state our uncertainty.
If I think something will happen with a 60% chance and it doesn't, I'm not nearly as "wrong" as if I'd predicted the event with a 99% chance.
Similarly though, if every day I predicted that the sun would rise the following morning with 60% probability, I would be wrong in a different sense: systematically underconfident.

Evaluating probability estimates isn't a new idea – I heard of it originally in Nate Silver's book *The Signal and the Noise*, which itself (if I recall correctly) traced the idea to *SuperForecasting* by Philip Tetlock.

FiveThirtyEight (formerly run by Silver) actually does annual ["calibrations"](https://projects.fivethirtyeight.com/checking-our-work/) in which they assess all of their predictions and basically make sure that of the things they said would happen 80% of the time, roughly 8/10 really did happen.
If more than 8/10 happened, then they were underconfident. If fewer, then they were overconfident.

This exercise is useful not just in checking your fancy statistical models but also in evaluating your own predictions, particularly in ensuring you don't have a bias toward overconfidence (which I suspect is far more common than the opposite).
I was inspired by Matt Yglesias' annual predictions and self evaluations[^slow-boring-calibration], so this year some friends and I will be "competing" by predicting various 2024 events.


See our estimates below – and I explained my own numbers in [this followup post](/feed/2023/12/25/probability-commentary-2024/).

{{< data/probability_estimate_table >}}

[^slow-boring-calibration]: Here's a [link](https://www.slowboring.com/p/a-look-back-at-my-predictions-for) but it's subscriber only.